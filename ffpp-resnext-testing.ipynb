{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4837965,"sourceType":"datasetVersion","datasetId":2803660},{"sourceId":5305898,"sourceType":"datasetVersion","datasetId":3067422}],"dockerImageVersionId":30445,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from distutils.dir_util import copy_tree\nfromDir = \"/kaggle/input/resnext-3-capsules-300x300\"\ndesDir = \"./\"\ncopy_tree(fromDir, desDir)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":11.223107,"end_time":"2023-02-10T03:23:18.248900","exception":false,"start_time":"2023-02-10T03:23:07.025793","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T18:19:17.776963Z","iopub.execute_input":"2023-04-03T18:19:17.777728Z","iopub.status.idle":"2023-04-03T18:19:21.051306Z","shell.execute_reply.started":"2023-04-03T18:19:17.777672Z","shell.execute_reply":"2023-04-03T18:19:21.050037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"papermill":{"duration":0.014588,"end_time":"2023-02-10T03:23:18.267903","exception":false,"start_time":"2023-02-10T03:23:18.253315","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T18:19:21.053481Z","iopub.execute_input":"2023-04-03T18:19:21.054367Z","iopub.status.idle":"2023-04-03T18:19:21.060934Z","shell.execute_reply.started":"2023-04-03T18:19:21.054318Z","shell.execute_reply":"2023-04-03T18:19:21.058911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.setrecursionlimit(15000)\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nimport torch.backends.cudnn as cudnn\nfrom torch.autograd import Variable\nimport torchvision.models as models\n\nNO_CAPS=3\n\nclass StatsNet(nn.Module):\n    def __init__(self):\n        super(StatsNet, self).__init__()\n\n    def forward(self, x):\n        x = x.view(x.data.shape[0], x.data.shape[1], x.data.shape[2]*x.data.shape[3])\n\n        mean = torch.mean(x, 2)\n        std = torch.std(x, 2)\n\n        return torch.stack((mean, std), dim=1)\n\nclass View(nn.Module):\n    def __init__(self, *shape):\n        super(View, self).__init__()\n        self.shape = shape\n\n    def forward(self, input):\n        return input.view(self.shape)\n\n\nclass VggExtractor(nn.Module):\n    def __init__(self, train=False):\n        super(VggExtractor, self).__init__()\n\n        self.vgg_1 = self.Vgg(torch.hub.load('pytorch/vision:v0.10.0', 'resnext50_32x4d', pretrained=True), 0, 4)\n        if train:\n            self.vgg_1.train(mode=True)\n            self.freeze_gradient()\n        else:\n            self.vgg_1.eval()\n\n    def Vgg(self, vgg, begin, end):\n        features = nn.Sequential(*list(vgg.children())[begin:(end+1)])\n        return features\n\n    def freeze_gradient(self, begin=0, end=9):\n        for i in range(begin, end+1):\n            self.vgg_1[i].requires_grad = False\n\n    def forward(self, input):\n        return self.vgg_1(input)\n\nclass FeatureExtractor(nn.Module):\n    def __init__(self):\n        super(FeatureExtractor, self).__init__()\n\n        self.capsules = nn.ModuleList([\n            nn.Sequential(\n                nn.Conv2d(256, 64, kernel_size=3, stride=1, padding=1),\n                nn.BatchNorm2d(64),\n                nn.ReLU(),\n                nn.Conv2d(64, 16, kernel_size=3, stride=1, padding=1),\n                nn.BatchNorm2d(16),\n                nn.ReLU(),\n                StatsNet(),\n\n                nn.Conv1d(2, 8, kernel_size=5, stride=2, padding=2),\n                nn.BatchNorm1d(8),\n                nn.Conv1d(8, 1, kernel_size=3, stride=1, padding=1),\n                nn.BatchNorm1d(1),\n                View(-1, 8),\n                )\n                for _ in range(NO_CAPS)]\n        )\n\n    def squash(self, tensor, dim):\n        squared_norm = (tensor ** 2).sum(dim=dim, keepdim=True)\n        scale = squared_norm / (1 + squared_norm)\n        return scale * tensor / (torch.sqrt(squared_norm))\n\n    def forward(self, x):\n        # outputs = [capsule(x.detach()) for capsule in self.capsules]\n        # outputs = [capsule(x.clone()) for capsule in self.capsules]\n        outputs = [capsule(x) for capsule in self.capsules]\n        output = torch.stack(outputs, dim=-1)\n\n        return self.squash(output, dim=-1)\n\nclass RoutingLayer(nn.Module):\n    def __init__(self, gpu_id, num_input_capsules, num_output_capsules, data_in, data_out, num_iterations):\n        super(RoutingLayer, self).__init__()\n\n        self.gpu_id = gpu_id\n        self.num_iterations = num_iterations\n        self.route_weights = nn.Parameter(torch.randn(num_output_capsules, num_input_capsules, data_out, data_in))\n\n\n    def squash(self, tensor, dim):\n        squared_norm = (tensor ** 2).sum(dim=dim, keepdim=True)\n        scale = squared_norm / (1 + squared_norm)\n        return scale * tensor / (torch.sqrt(squared_norm))\n\n    def forward(self, x, random, dropout):\n        # x[b, data, in_caps]\n\n        x = x.transpose(2, 1)\n        # x[b, in_caps, data]\n\n        if random:\n            noise = Variable(0.01*torch.randn(*self.route_weights.size()))\n            if self.gpu_id >= 0:\n                noise = noise.cuda(self.gpu_id)\n            route_weights = self.route_weights + noise\n        else:\n            route_weights = self.route_weights\n\n        priors = route_weights[:, None, :, :, :] @ x[None, :, :, :, None]\n\n        # route_weights [out_caps , 1 , in_caps , data_out , data_in]\n        # x             [   1     , b , in_caps , data_in ,    1    ]\n        # priors        [out_caps , b , in_caps , data_out,    1    ]\n\n        priors = priors.transpose(1, 0)\n        # priors[b, out_caps, in_caps, data_out, 1]\n\n        if dropout > 0.0:\n            drop = Variable(torch.FloatTensor(*priors.size()).bernoulli(1.0- dropout))\n            if self.gpu_id >= 0:\n                drop = drop.cuda(self.gpu_id)\n            priors = priors * drop\n            \n\n        logits = Variable(torch.zeros(*priors.size()))\n        # logits[b, out_caps, in_caps, data_out, 1]\n\n        if self.gpu_id >= 0:\n            logits = logits.cuda(self.gpu_id)\n\n        num_iterations = self.num_iterations\n\n        for i in range(num_iterations):\n            probs = F.softmax(logits, dim=2)\n            outputs = self.squash((probs * priors).sum(dim=2, keepdim=True), dim=3)\n\n            if i != self.num_iterations - 1:\n                delta_logits = priors * outputs\n                logits = logits + delta_logits\n\n        # outputs[b, out_caps, 1, data_out, 1]\n        outputs = outputs.squeeze()\n\n        if len(outputs.shape) == 3:\n            outputs = outputs.transpose(2, 1).contiguous() \n        else:\n            outputs = outputs.unsqueeze_(dim=0).transpose(2, 1).contiguous()\n        # outputs[b, data_out, out_caps]\n\n        return outputs\n\n\nclass CapsuleNet(nn.Module):\n    def __init__(self, num_class, gpu_id):\n        super(CapsuleNet, self).__init__()\n\n        self.num_class = num_class\n        self.fea_ext = FeatureExtractor()\n        self.fea_ext.apply(self.weights_init)\n\n        self.routing_stats = RoutingLayer(gpu_id=gpu_id, num_input_capsules=NO_CAPS, num_output_capsules=num_class, data_in=8, data_out=4, num_iterations=2)\n\n    def weights_init(self, m):\n        classname = m.__class__.__name__\n        if classname.find('Conv') != -1:\n            m.weight.data.normal_(0.0, 0.02)\n        elif classname.find('BatchNorm') != -1:\n            m.weight.data.normal_(1.0, 0.02)\n            m.bias.data.fill_(0)\n\n    def forward(self, x, random=False, dropout=0.0):\n\n        z = self.fea_ext(x)\n        z = self.routing_stats(z, random, dropout=dropout)\n        # z[b, data, out_caps]\n\n        # classes = F.softmax(z, dim=-1)\n\n        # class_ = classes.detach()\n        # class_ = class_.mean(dim=1)\n\n        # return classes, class_\n\n        classes = F.softmax(z, dim=-1)\n        class_ = classes.detach()\n        class_ = class_.mean(dim=1)\n\n        return z, class_\n\n\nclass CapsuleLoss(nn.Module):\n    def __init__(self, gpu_id):\n        super(CapsuleLoss, self).__init__()\n        self.cross_entropy_loss = nn.CrossEntropyLoss()\n\n        if gpu_id >= 0:\n            self.cross_entropy_loss.cuda(gpu_id)\n\n    def forward(self, classes, labels):\n        loss_t = self.cross_entropy_loss(classes[:,0,:], labels)\n\n        for i in range(classes.size(1) - 1):\n            loss_t = loss_t + self.cross_entropy_loss(classes[:,i+1,:], labels)\n\n        return loss_t","metadata":{"papermill":{"duration":2.218311,"end_time":"2023-02-10T03:23:20.489918","exception":false,"start_time":"2023-02-10T03:23:18.271607","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T18:19:21.062972Z","iopub.execute_input":"2023-04-03T18:19:21.063434Z","iopub.status.idle":"2023-04-03T18:19:23.968924Z","shell.execute_reply.started":"2023-04-03T18:19:21.063388Z","shell.execute_reply":"2023-04-03T18:19:23.967623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.setrecursionlimit(15000)\nimport os\nimport torch\nimport torch.backends.cudnn as cudnn\nimport numpy as np\nfrom torch.autograd import Variable\nimport torch.utils.data\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nfrom tqdm import tqdm\nimport argparse\nfrom sklearn import metrics\nfrom scipy.optimize import brentq\nfrom scipy.interpolate import interp1d\nfrom sklearn.metrics import roc_curve","metadata":{"papermill":{"duration":0.906095,"end_time":"2023-02-10T03:23:21.398766","exception":false,"start_time":"2023-02-10T03:23:20.492671","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T18:19:23.971275Z","iopub.execute_input":"2023-04-03T18:19:23.971819Z","iopub.status.idle":"2023-04-03T18:19:24.378371Z","shell.execute_reply.started":"2023-04-03T18:19:23.971772Z","shell.execute_reply":"2023-04-03T18:19:24.377147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = \"/kaggle/input/ffpp-labeled/FFPPx100\"\ntest_set = \"test\"\nworkers = 4\nbatchSize = 16\nimageSize = 300\ngpu_id = -1\noutf = \"./\"\nrandom = False\nid = 47","metadata":{"papermill":{"duration":0.012819,"end_time":"2023-02-10T03:23:21.414455","exception":false,"start_time":"2023-02-10T03:23:21.401636","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T18:25:01.429393Z","iopub.execute_input":"2023-04-03T18:25:01.429891Z","iopub.status.idle":"2023-04-03T18:25:01.436977Z","shell.execute_reply.started":"2023-04-03T18:25:01.429851Z","shell.execute_reply":"2023-04-03T18:25:01.435555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_writer = open(os.path.join(outf, 'test.txt'), 'a+')\n\ntransform_fwd = transforms.Compose([\n    transforms.Resize((imageSize, imageSize)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n    ])\n\n\ndataset_test = dset.ImageFolder(root=os.path.join(dataset, test_set), transform=transform_fwd)\nassert dataset_test\ndataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=batchSize, shuffle=False, num_workers=int(workers))\n\nvgg_ext = VggExtractor()\ncapnet = CapsuleNet(2, gpu_id)\n\ncapnet.load_state_dict(torch.load(os.path.join(outf,'capsule_' + str(id) + '.pt'), map_location=torch.device('cpu')))\ncapnet.eval()\n\nif gpu_id >= 0:\n    vgg_ext.cuda(gpu_id)\n    capnet.cuda(gpu_id)\n\n\n##################################################################################\n\ntol_label = np.array([], dtype=np.float)\ntol_pred = np.array([], dtype=np.float)\ntol_pred_prob = np.array([], dtype=np.float)\n\ncount = 0\nloss_test = 0\n\nfor img_data, labels_data in dataloader_test:\n\n    labels_data[labels_data > 1] = 1\n    img_label = labels_data.numpy().astype(np.float)\n\n    if gpu_id >= 0:\n        img_data = img_data.cuda(gpu_id)\n        labels_data = labels_data.cuda(gpu_id)\n\n    input_v = Variable(img_data)\n\n    x = vgg_ext(input_v)\n    classes, class_ = capnet(x, random=random)\n\n    output_dis = class_.data.cpu()\n    output_pred = np.zeros((output_dis.shape[0]), dtype=np.float)\n\n    for i in range(output_dis.shape[0]):\n        if output_dis[i,1] >= output_dis[i,0]:\n            output_pred[i] = 1.0\n        else:\n            output_pred[i] = 0.0\n\n    tol_label = np.concatenate((tol_label, img_label))\n    tol_pred = np.concatenate((tol_pred, output_pred))\n\n    pred_prob = torch.softmax(output_dis, dim=1)\n    tol_pred_prob = np.concatenate((tol_pred_prob, pred_prob[:,1].data.numpy()))\n\n    count += 1\n\nacc_test = metrics.accuracy_score(tol_label, tol_pred)\nloss_test /= count\n\nfpr, tpr, thresholds = roc_curve(tol_label, tol_pred_prob, pos_label=1)\neer = brentq(lambda x : 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n\n# fnr = 1 - tpr\n# hter = (fpr + fnr)/2\n\nprint('[Epoch %d] Test acc: %.2f   EER: %.2f' % (id, acc_test*100, eer*100))\ntext_writer.write('%d,%.2f,%.2f\\n'% (id, acc_test*100, eer*100))\n\ntext_writer.flush()\ntext_writer.close()","metadata":{"papermill":{"duration":598.979156,"end_time":"2023-02-10T03:33:20.396468","exception":false,"start_time":"2023-02-10T03:23:21.417312","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T18:25:43.926946Z","iopub.execute_input":"2023-04-03T18:25:43.927454Z"},"trusted":true},"execution_count":null,"outputs":[]}]}